import pickle
from typing import Tuple

import jax
import jax.numpy as jnp
from flax.struct import dataclass

from rddp.tasks.base import OptimalControlProblem


@dataclass
class AnnealedLangevinOptions:
    """Parameters for annealed Langevin dynamics.

    Annealed Langevin dynamics samples from the target distribution

        p(U | xâ‚€) âˆ exp(-J(U | xâ‚€) / Î»),

    by considering intermediate noised distributions

        pâ‚–(U | xâ‚€) = âˆ« p(UÌƒ | xâ‚€)N(UÌƒ;U,Ïƒâ‚–Â²)dUÌƒ

    with a geometrically decreasing sequence of noise levels k = L, L-1, ..., 0.

    Attributes:
        temperature: The temperature Î»
        num_noise_levels: The number of noise levels L.
        starting_noise_level: The starting noise level Ïƒ_L.
        noise_decay_rate: The noise decay rate Ïƒâ‚–â‚‹â‚ = Î³ Ïƒâ‚–.
        num_steps: The number of Langevin steps to take at each noise level, N.
        step_size: The Langevin step size Î±.
    """

    temperature: float
    num_noise_levels: int
    starting_noise_level: int
    noise_decay_rate: float
    num_steps: int
    step_size: float


@dataclass
class DatasetGenerationOptions:
    """Parameters for generating a diffusion policy dataset.

    Attributes:
        num_initial_states: The number of initial states xâ‚€ to sample.
        num_rollouts_per_data_point: The number of rollouts used to estimate
                                     each score, M.
    """

    num_initial_states: int
    num_rollouts_per_data_point: int


@dataclass
class DiffusionDataset:
    """Training data for a diffusion policy.

    Attributes:
        x0: The initial state xâ‚€.
        U: The control sequence U = [uâ‚€, uâ‚, ..., u_Tâ‚‹â‚].
        s: The noised score estimate sÌ‚ = âˆ‡ log pâ‚–(U | xâ‚€).
        sigma: The noise level Ïƒâ‚–.
    """

    x0: jnp.ndarray
    U: jnp.ndarray
    s: jnp.ndarray
    sigma: jnp.ndarray


class DatasetGenerator:
    """Generate a diffusion policy dataset for score function learning.

    The dataset consists of tuples
        (xâ‚€, U, sÌ‚, k, Ïƒâ‚–),
    where
        xâ‚€ is the initial state,
        U is the control sequence U = [uâ‚€, uâ‚, ..., u_Tâ‚‹â‚],
        sÌ‚ is the noised score estimate sÌ‚ = âˆ‡ log pâ‚–(U | xâ‚€),
        and k is noise level index.
    """

    def __init__(
        self,
        prob: OptimalControlProblem,
        langevin_options: AnnealedLangevinOptions,
        datagen_options: DatasetGenerationOptions,
    ):
        """Initialize the dataset generator.

        Args:
            prob: The optimal control problem defining the cost J(U | xâ‚€).
            langevin_options: Sampling (e.g., temperature) settings.
            datagen_options: Dataset generation (e.g., num rollouts) settings.
        """
        self.prob = prob
        self.langevin_options = langevin_options
        self.datagen_options = datagen_options

    def estimate_noised_score(
        self,
        x0: jnp.ndarray,
        controls: jnp.ndarray,
        sigma: float,
        rng: jax.random.PRNGKey,
    ) -> jnp.ndarray:
        """Estimate the noised score s = âˆ‡ log pâ‚–(U | xâ‚€) with M rollouts.

        The score of the noised target distribution

            pâ‚–(U | xâ‚€) = âˆ« p(UÌƒ | xâ‚€)N(UÌƒ;U,Ïƒâ‚–Â²)dUÌƒ,
            p(U | xâ‚€) âˆ exp(-J(U | xâ‚€) / Î»),

        is given by

            ÏƒÂ² âˆ‡ log pâ‚–(U | xâ‚€) =
                ð”¼[exp(-J(UÌƒ | xâ‚€) / Î»)(UÌƒ - U)] / ð”¼[exp(-J(UÌƒ | xâ‚€) / Î»)],

        where the expectation is under UÌƒ ~ ð’©(U,Ïƒâ‚–Â²).

        Args:
            x0: The initial state xâ‚€.
            controls: The control sequence U = [uâ‚€, uâ‚, ..., u_Tâ‚‹â‚].
            sigma: The noise level Ïƒâ‚–.
            rng: The random number generator key.

        Returns:
            The noised score estimate sÌ‚ = ÏƒÂ² âˆ‡ log pâ‚–(U | xâ‚€).
        """
        M = self.datagen_options.num_rollouts_per_data_point
        lmbda = self.langevin_options.temperature

        # Sample control tapes UÌƒÊ² ~ ð’©(U,ÏƒÂ²)
        rng, ctrl_rng = jax.random.split(rng)
        U_noised = controls + sigma * jax.random.normal(
            ctrl_rng, (M, *controls.shape)
        )

        # Compute the cost of each control tape
        J = jax.vmap(self.prob.total_cost, in_axes=(0, None))(U_noised, x0)
        J = J - jnp.min(J, axis=0)  # normalize for better numerics

        # Compute importance weights
        weights = jnp.exp(-J / lmbda)
        weights = weights / (jnp.sum(weights, axis=0) + 1e-6)  # avoid / 0

        # Compute the noised score estimate
        deltaU = U_noised - controls
        score_estimate = jnp.einsum("i,i...->...", weights, deltaU)
        score_estimate /= sigma**2

        return score_estimate

    def generate_from_state(
        self, x0: jnp.ndarray, rng: jax.random.PRNGKey
    ) -> DiffusionDataset:
        """Generate a dataset of noised score estimates from one initial state.

        Starting from initial state xâ‚€:
          - Sample a control tape Î¼_L = [uâ‚€, uâ‚, ..., u_Tâ‚‹â‚] ~ ð’©(0, Ïƒ_LÂ²)
          - For each noise level k = L, L-1, ..., 0:
              - Sample Uâ‚–â± ~ ð’©(Î¼â‚–, Ïƒâ‚–Â²), i = 1..N
              - Estimate noised score sÌ‚ = Ïƒâ‚–Â² âˆ‡ log pâ‚–(Uâ‚–â± | xâ‚€) with M rollouts
              - Add (xâ‚€, Uâ‚–â±, sÌ‚â‚–â±, k) to the dataset
              - Update the mean control tape Î¼â‚–â‚‹â‚ = Î¼â‚–â‚‹â‚ + 1/N âˆ‘áµ¢ sÌ‚â‚–â±

        By the end of this process, Î¼â‚€ should be close to a local optimum.

        Args:
            x0: The initial state xâ‚€.
            rng: The random number generator key.

        Returns:
            Dataset of states, controls, scores, and noise levels (xâ‚€, U, sÌ‚, k).
        """
        L = self.langevin_options.num_noise_levels
        N = self.langevin_options.num_steps
        sigmaL = self.langevin_options.starting_noise_level
        gamma = self.langevin_options.noise_decay_rate
        alpha = self.langevin_options.step_size

        def langevin_step(carry: Tuple, i: int):
            """Perform a single Langevin sampling step at noise level sigma.

            Return the new control tape Uâ‚–â±âºÂ¹ and the score estimate sÌ‚â‚–â±.
            """
            U, sigma, rng = carry
            rng, score_rng, z_rng = jax.random.split(rng, 3)
            eps = alpha * sigma ** 2

            # Langevin dynamics based on the estimated score
            z = jax.random.normal(z_rng, U.shape)
            s = self.estimate_noised_score(x0, U, sigma, score_rng)
            U_new = U + eps * s + jnp.sqrt(2 * eps) * z

            # Record training data
            data = DiffusionDataset(
                x0=x0,
                U=U,
                s=s,
                sigma=jnp.array([sigma])
            )

            return (U_new, sigma, rng), data

        def annealed_langevin_step(carry: Tuple, k: int):
            """Generate samples at the k-th noise level."""
            (U, sigma, rng) = carry

            # Run Langevin dynamics for N steps, recording score estimates 
            # along the way
            rng, langevin_rng = jax.random.split(rng)
            (U, _, _), data = jax.lax.scan(langevin_step,
                                           (U, sigma, langevin_rng), jnp.arange(N))
            
            # Reduce the noise level Ïƒâ‚–â‚‹â‚ = Î³ Ïƒâ‚–
            sigma *= gamma

            return (U, sigma, rng), data
        
        # Sample U ~ ð’©(0, Ïƒ_LÂ²)
        rng, mu_rng = jax.random.split(rng)
        U = sigmaL * jax.random.normal(
            mu_rng, (self.prob.num_steps - 1, *self.prob.sys.action_shape)
        )

        # Generate data for each noise level
        rng, sampling_rng = jax.random.split(rng)
        _, dataset = jax.lax.scan(annealed_langevin_step,
                                  (U, sigmaL, sampling_rng),  jnp.arange(L - 1, -1, -1))

        return dataset

    def generate(self, rng: jax.random.PRNGKey) -> DiffusionDataset:
        """Generate a dataset of noised score estimates, (xâ‚€, U, sÌ‚, k).

        Data is generated for various initial conditions and noise levels, but
        flattened into a single dataset with shape [sample, data].

        Args:
            x0: The initial state xâ‚€.
            rng: The random number generator key.

        Returns:
            Dataset of states, controls, scores, and noise levels (xâ‚€, U, sÌ‚, k).
        """
        # Sample initial states
        rng, state_rng = jax.random.split(rng)
        state_rng = jax.random.split(
            state_rng, self.datagen_options.num_initial_states
        )
        x0s = jax.vmap(self.prob.sample_initial_state)(state_rng)

        # Generate data for each initial state
        rng, gen_rng = jax.random.split(rng)
        gen_rng = jax.random.split(
            gen_rng, self.datagen_options.num_initial_states
        )
        stacked_data = jax.vmap(self.generate_from_state)(x0s, gen_rng)

        # Flatten the data across initial states, noise levels, and data points.
        flat_data = jax.tree.map(
            lambda x: jnp.reshape(x, (-1, *x.shape[3:])), stacked_data
        )

        return flat_data

    def save_dataset(self, dataset: DiffusionDataset, path: str) -> None:
        """Save a training dataset to a file.

        Note that this also saves the annealed langevin options, which are
        needed for depolying a trained score model.

        Args:
            dataset: The dataset to save.
            path: The path to save the dataset to.
        """
        data = {"dataset": dataset, "langevin_options": self.langevin_options}
        with open(path, "wb") as f:
            pickle.dump(data, f)
