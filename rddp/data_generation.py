from typing import Tuple

import jax
import jax.numpy as jnp
from flax.struct import dataclass

from rddp.tasks.base import OptimalControlProblem


@dataclass
class AnnealedLangevinOptions:
    """Parameters for annealed Langevin dynamics.

    Annealed Langevin dynamics samples from the target distribution

        p(U | x‚ÇÄ) ‚àù exp(-J(U | x‚ÇÄ) / Œª),

    by considering intermediate noised distributions

        p‚Çñ(U | x‚ÇÄ) = ‚à´ p(UÃÉ | x‚ÇÄ)N(UÃÉ;U,œÉ‚Çñ¬≤)dUÃÉ

    with a geometrically decreasing sequence of noise levels k = L, L-1, ..., 0.

    Attributes:
        temperature: The temperature Œª
        num_noise_levels: The number of noise levels L.
        starting_noise_level: The starting noise level œÉ_L.
        noise_decay_rate: The noise decay rate œÉ‚Çñ‚Çã‚ÇÅ = Œ≥ œÉ‚Çñ.
    """

    temperature: float
    num_noise_levels: int
    starting_noise_level: int
    noise_decay_rate: float


@dataclass
class DatasetGenerationOptions:
    """Parameters for generating a diffusion policy dataset.

    Attributes:
        num_initial_states: The number of initial states x‚ÇÄ to sample.
        num_data_points_per_initial_state: The number of data points per initial
                                           state, N.
        num_rollouts_per_data_point: The number of rollouts used to estimate
                                     each score, M.
    """

    num_initial_states: int
    num_data_points_per_initial_state: int
    num_rollouts_per_data_point: int


class DatasetGenerator:
    """Generate a diffusion policy dataset for score function learning.

    The dataset consists of tuples
        (x‚ÇÄ, U, sÃÇ, k),
    where
        x‚ÇÄ is the initial state,
        U is the control sequence U = [u‚ÇÄ, u‚ÇÅ, ..., u_T‚Çã‚ÇÅ],
        sÃÇ is the noised score estimate sÃÇ = œÉ‚Çñ¬≤ ‚àá log p‚Çñ(U | x‚ÇÄ),
        and k is noise level index.
    """

    def __init__(
        self,
        prob: OptimalControlProblem,
        langevin_options: AnnealedLangevinOptions,
        datagen_options: DatasetGenerationOptions,
    ):
        """Initialize the dataset generator.

        Args:
            prob: The optimal control problem defining the cost J(U | x‚ÇÄ).
            langevin_options: Sampling (e.g., temperature) settings.
            datagen_options: Dataset generation (e.g., num rollouts) settings.
        """
        self.prob = prob
        self.langevin_options = langevin_options
        self.datagen_options = datagen_options

    def estimate_noised_score(
        self,
        x0: jnp.ndarray,
        controls: jnp.ndarray,
        sigma: float,
        rng: jax.random.PRNGKey,
    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:
        """Estimate the noised score s = œÉ¬≤ ‚àá log p‚Çñ(U | x‚ÇÄ) with M rollouts.

        The score of the noised target distribution

            p‚Çñ(U | x‚ÇÄ) = ‚à´ p(UÃÉ | x‚ÇÄ)N(UÃÉ;U,œÉ‚Çñ¬≤)dUÃÉ,
            p(U | x‚ÇÄ) ‚àù exp(-J(U | x‚ÇÄ) / Œª),

        is given by

            œÉ¬≤ ‚àá log p‚Çñ(U | x‚ÇÄ) =
                ùîº[exp(-J(UÃÉ | x‚ÇÄ) / Œª)(UÃÉ - U)] / ùîº[exp(-J(UÃÉ | x‚ÇÄ) / Œª)],

        where the expectation is under UÃÉ ~ ùí©(U,œÉ‚Çñ¬≤).

        Args:
            x0: The initial state x‚ÇÄ.
            controls: The control sequence U = [u‚ÇÄ, u‚ÇÅ, ..., u_T‚Çã‚ÇÅ].
            sigma: The noise level œÉ‚Çñ.
            rng: The random number generator key.

        Returns:
            The noised score estimate sÃÇ = œÉ¬≤ ‚àá log p‚Çñ(U | x‚ÇÄ).
            The sampled control tapes UÃÉ ≤, j = 1..M.
            The importance weights w‚Çñ(UÃÉ ≤).
        """
        M = self.datagen_options.num_rollouts_per_data_point
        lmbda = self.langevin_options.temperature

        # Sample control tapes UÃÉ ≤ ~ ùí©(U,œÉ¬≤)
        rng, ctrl_rng = jax.random.split(rng)
        U_noised = controls + sigma * jax.random.normal(
            ctrl_rng, (M, *controls.shape)
        )

        # Compute the cost of each control tape
        J = jax.vmap(self.prob.total_cost, in_axes=(0, None))(U_noised, x0)
        J = J - jnp.min(J, axis=0)  # normalize for better numerics

        # Compute importance weights
        weights = jnp.exp(-J / lmbda)
        weights = weights / (jnp.sum(weights, axis=0) + 1e-6)  # avoid / 0

        # Compute the noised score estimate
        deltaU = U_noised - controls
        score_estimate = jnp.einsum("i,i...->...", weights, deltaU)

        return score_estimate, U_noised, weights

    def generate_from_state(
        self, x0: jnp.ndarray, rng: jax.random.PRNGKey
    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:
        """Generate a dataset of noised score estimates from one initial state.

        Starting from initial state x‚ÇÄ:
          - Sample a control tape Œº_L = [u‚ÇÄ, u‚ÇÅ, ..., u_T‚Çã‚ÇÅ] ~ ùí©(0, œÉ_L¬≤)
          - For each noise level k = L, L-1, ..., 0:
              - Sample U‚Çñ‚Å± ~ ùí©(Œº‚Çñ, œÉ‚Çñ¬≤), i = 1..N
              - Estimate noised score sÃÇ = œÉ‚Çñ¬≤ ‚àá log p‚Çñ(U‚Çñ‚Å± | x‚ÇÄ) with M rollouts
              - Add (x‚ÇÄ, U‚Çñ‚Å±, sÃÇ, k) to the dataset
              - Update the mean control tape Œº‚Çñ‚Çã‚ÇÅ = MPPI(U‚Çñ‚Å± ≤)

        By the end of this process, Œº‚ÇÄ should be close to a local optimum.

        Args:
            x0: The initial state x‚ÇÄ.
            rng: The random number generator key.

        Returns:
            Dataset of states, controls, scores, and noise levels (x‚ÇÄ, U, sÃÇ, k).
        """
        sigma = self.langevin_options.starting_noise_level
        L = self.langevin_options.num_noise_levels
        N = self.datagen_options.num_data_points_per_initial_state

        # Sample Œº_L ~ ùí©(0, œÉ_L¬≤)
        rng, mu_rng = jax.random.split(rng)
        mu = sigma * jax.random.normal(
            mu_rng, (self.prob.num_steps - 1, *self.prob.sys.action_shape)
        )

        for k in range(L - 1, -1, -1):
            print("")
            print("k =", k)
            # Sample N control tapes U‚Çñ‚Å± ~ ùí©(Œº‚Çñ, œÉ‚Çñ¬≤)
            rng, ctrl_rng = jax.random.split(rng)
            U = mu + sigma * jax.random.normal(ctrl_rng, (N, *mu.shape))

            # Estimate noised scores sÃÇ = œÉ‚Çñ¬≤ ‚àá log p‚Çñ(U | x‚ÇÄ) with M rollouts
            rng, score_rng = jax.random.split(rng)
            score_rng = jax.random.split(score_rng, N)

            s, U_noised, weights = jax.vmap(
                self.estimate_noised_score, in_axes=(None, 0, None, 0)
            )(x0, U, sigma, score_rng)

            # Update Œº‚Çñ‚Çã‚ÇÅ = MPPI(U‚Çñ‚Å± ≤)
            # TODO: figure out a better/more principled thing to do here
            mu = jnp.einsum("ij,ij...->...", weights, U_noised) / N

            print("sigma:", sigma)
            print("cost:", self.prob.total_cost(mu, x0))

            # Update œÉ‚Çñ‚Çã‚ÇÅ = Œ≥ œÉ‚Çñ
            sigma *= self.langevin_options.noise_decay_rate
